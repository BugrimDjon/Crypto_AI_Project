# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
import joblib
from tensorflow.keras.models import load_model, Sequential
from enums.coins import Coins
from database.db import Database
from services.okx_candles import OkxCandlesFetcher
from services.time_control import TimControl
from enums.timeframes import Timeframe
from config.SettingsCoins import SettingsCoins
from enums.AfterBefore import AfterBefore
from logger.context_logger import ContextLogger
from AI.AIModelService import AIModelService
from datetime import datetime
from AI.ExperimentManager import ExperimentManager
from collections import defaultdict
from ModelManager.ModelManager import ModelManager
from datetime import datetime
import tzlocal
from datetime import timedelta
from Reports.reports import Reports
from MathCandles.mathCandles import MathCandles
import tensorflow as tf
import optuna
import gc
from joblib import parallel_backend


import pandas as pd
import ta
import json
import logging
import os, psutil
import sys
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,  # –∏–ª–∏ DEBUG, WARNING, ERROR
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("app.log", encoding="utf-8"),  # –ü–∏—Å–∞—Ç—å –≤ —Ñ–∞–π–ª
        logging.StreamHandler(),  # –ü–∏—Å–∞—Ç—å –≤ –∫–æ–Ω—Å–æ–ª—å
    ],
)


class Servise:
    def __init__(self, db: Database) -> None:
        self.db = db
        self.ai_service = AIModelService(db)
        self.reports_servise=Reports(db)

    @staticmethod
    def print_full_mem_info(tag=""):
        process = psutil.Process(os.getpid())
        mem = process.memory_info().rss / 1024 / 1024
        swap = psutil.swap_memory()
        print(f"[{tag}] RAM: {mem:.2f} MB | SWAP: {swap.used / 1024 / 1024:.2f} MB")



    def run_optuna_search(self, df_ready, n_trials=100):
        def objective(trial):
            # print(f"–ó–∞–ø—É—Å–∫ trial –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ PID: {os.getpid()}")
            self.print_full_mem_info(tag="–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏ (–Ω–∞—á–∞–ª–æ trial)")
            
            learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-4, log=True)
            dropout = trial.suggest_float("dropout", 0.001, 0.1)
            neyro = trial.suggest_categorical("neyro", [64,128,256])
            batch_size=trial.suggest_categorical("batch_size",[32, 64, 128])
            window_size=trial.suggest_categorical("window_size",[180, 240])
                                                  
            # batch_size = 64 if neyro > 300 else 128

            manager = ExperimentManager(self.ai_service)
            # print ("–æ–ø—Ç—É–Ω–∞ –∑–∞–ø—É—Å–∫–∞–µ—Ç run_experiment")
            mae, rmse = manager.run_experiment(
                table_name=Coins.FET,
                timeframe=Timeframe._4hour,
                window_size=window_size,
                horizon=24,
                epochs=70,
                learning_rate=learning_rate,
                dropout=dropout,
                neyro=neyro,
                df_ready=df_ready,
                offset=10,
                batch_size=batch_size,
            )
            tf.keras.backend.clear_session()
            del manager
            gc.collect()
            self.print_full_mem_info(tag="–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏)")

            return rmse  # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ mae –∏–ª–∏ val_loss

        storage_path = "results/fet_optuna.db"
        storage_url = f"sqlite:///{storage_path}"
        
        
        study = optuna.create_study(
            direction="minimize",
            pruner=optuna.pruners.MedianPruner(),  # –û—Ç—Å–µ—á–µ–Ω–∏–µ –ø–ª–æ—Ö–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            study_name="fet_study",
            storage=storage_url,
            load_if_exists=True
        )

        def after_trial_callback(study, trial):
                # # time.sleep(2)
                # if (trial.number + 1) % 4 == 0:
                #     print(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Å–ª–µ trial #{trial.number + 1}")
                #     sys.exit(0)
                #     # study.stop()
            x=0
            

        study.optimize(objective, n_trials=n_trials, n_jobs=1,  callbacks=[after_trial_callback])
            # logging.debug

        print("\nü•á –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:", study.best_params)
        return study


    def ai_expirement(self,use_optuna=False):
        import tensorflow as tf
        # print("–∑–∞—à–ª–∏ –≤ ai_expirement ")
        tf.config.threading.set_intra_op_parallelism_threads(4)
        tf.config.threading.set_inter_op_parallelism_threads(4)
        current_tf=Timeframe._4hour
        current_coins=Coins.FET
        offset=10
        table_name=current_coins
        limit=1000000
        
        query = f""" SELECT * FROM {table_name.value}
                            WHERE timeFrame=%s
                            ORDER BY ts DESC
                            LIMIT %s;"""
        params = (Timeframe._1min.label,limit)
        rows = self.db.query_to_bd(query, params)
        columns = [
                    "ts", "o", "h", "l", "c", "vol", "volCcy", "volCcyQuote"
                ]
        print ("—Å—á–∏—Ç–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î")
        df_1min = pd.DataFrame(rows, columns=columns)
        df_1min = df_1min.sort_values("ts").reset_index(drop=True)
        amount=list(range(0,current_tf.minutes,offset))
        math_candle=MathCandles()

        df = math_candle.generate_multi_shift_features(df_1min,current_tf , amount)
        df = df.sort_values("ts").reset_index(drop=True)
        del df_1min
        # print("–ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ")
        
        # ‚úÖ –£–∫–∞–∂–∏ –ø—É—Ç—å, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
        # save_path = "D:/Project_programming/for_AI/Crypto_AI_Project/Colab/df_ready.pkl"  # –µ—Å–ª–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å Google Drive

        if use_optuna:
            # from optuna_exp import run_optuna_search
            self.run_optuna_search(df_ready=df)
            return  # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø–æ—Å–ª–µ Optuna


        counter=0
        manager = ExperimentManager(self.ai_service)
        for window in [240]: #[30, 45]
            for horizon in [24]:   #[1, 2]
                for learning_rate in [0.00025,0.0001,0.00005]: #[0.0005, 0.0001]
                    for dropout in [0.005, 0.01, 0.05]: #[0.01, 0.05]:
                        for neyro in [64,128,256, 384]:     #[128, 256]:
                            counter+=1
                            print(f"–ü—Ä–æ—Ö–æ–¥ - {counter}")
                            # tf.debugging.set_log_device_placement(True)
                            if (counter<0):  #613
                                continue
                            
                            if neyro>300:
                                batch_size=32
                            else:
                                batch_size=64
                            manager.run_experiment(
                                table_name=current_coins,
                                timeframe=current_tf,
                                window_size=window,
                                horizon=horizon,
                                epochs=70,
                                learning_rate=learning_rate,
                                dropout=dropout,
                                neyro=neyro,
                                df_ready=df,
                                offset=offset,
                                batch_size=batch_size,
                            )
                            tf.keras.backend.clear_session()
        manager.plot_results()





    def update_y_true(csv_path: str, actual_prices: pd.DataFrame):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç y_true, CSV-—Ñ–∞–π–ª–µ —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        :param csv_path: –ü—É—Ç—å –∫ CSV —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏.
        :param actual_prices: DataFrame —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
            –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: ['ts', 'c'] ‚Äî –º–µ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è.
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        df = pd.read_csv(csv_path, parse_dates=['target_ts', 'eval_time'])

        # –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø–æ–Ω–∏–∂–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä –∏ —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–µ–µ
        actual_prices = actual_prices[['ts', 'c']].dropna().copy()
        actual_prices.columns = ['ts', 'y_true']

        updated_rows = 0

        # –ü—Ä–æ–π–¥—ë–º –ø–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º, –≥–¥–µ y_true –µ—â—ë –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω
        for i, row in df[df['y_true'].isna()].iterrows():
            target_ts = pd.to_datetime(row['target_ts'])

            # –ù–∞–π—Ç–∏ –≤ actual_prices –Ω—É–∂–Ω—É—é —Ç–æ—á–∫—É
            y_row = actual_prices[actual_prices['ts'] == target_ts]

            if not y_row.empty:
                y_true = y_row['y_true'].values[0]
                y_pred = row['y_pred']

                df.at[i, 'y_true'] = y_true
                updated_rows += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        df.to_csv(csv_path, index=False)
        print(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: {updated_rows}")

    def make_forecast_on_working_models(self):
        manager = ModelManager()
        math_candle=MathCandles()

        table_name = Coins.FET
        time_frame= Timeframe._1min
        model_results = []

        model_results_df = manager.list_models_out_df()
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ Timeframe
        model_results_df["timeframe_enum"] = model_results_df["timeframe"].apply(lambda x: Timeframe[x])

        # –¢–µ–ø–µ—Ä—å —Ç—ã –º–æ–∂–µ—à—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∏–Ω—É—Ç:
        model_results_df["tf_minutes"] = model_results_df["timeframe_enum"].apply(lambda tf: tf.minutes)

        # –ò–ª–∏ –º–µ—Ç–∫—É:
        model_results_df["tf_label"] = model_results_df["timeframe_enum"].apply(lambda tf: tf.label)

        # model_results_df.sort_values(by="tf_minutes", ascending=True, inplace=True)
        model_results_df.sort_values(by=["tf_minutes", "offset"], ascending=[True, False], inplace=True)

        expr = (
            (model_results_df["window_size"] + model_results_df["horizon"]) *
            (model_results_df["tf_minutes"] / model_results_df["offset"])
            )
        max_idx = expr.idxmax()
        row_max_ws=model_results_df.loc[max_idx]

        limit=int((expr.loc[max_idx]+(300*row_max_ws["tf_minutes"])))
        query = f""" SELECT * FROM {table_name.value}
                            WHERE timeFrame=%s
                            ORDER BY ts DESC
                            LIMIT %s;"""
        params = (Timeframe._1min.label,limit)
        rows = self.db.query_to_bd(query, params)
        columns = [
                    "ts", "o", "h", "l", "c", "vol", "volCcy", "volCcyQuote"
                ]
        df_1min = pd.DataFrame(rows, columns=columns)
        df_1min = df_1min.sort_values("ts").reset_index(drop=True)
        # –ü–æ–ª—É—á–∏–º 15-–º–∏–Ω —Å–≤–µ—á–∏ –±–µ–∑ —Å–º–µ—â–µ–Ω–∏—è
        # df_15min = math_candle.aggregate_with_offset(df_1min, Timeframe._15min, offset_minutes=0)
       
#  if id_row==0 or time_frame!=row["timeframe_enum"]:
#                 time_frame=row["timeframe_enum"]

#                 query = f""" SELECT * FROM {table_name.value}
#                             WHERE timeFrame=%s
#                             ORDER BY ts DESC
#                             LIMIT %s;"""
#                 params = (time_frame.label,limit)
            
#                 rows = self.db.query_to_bd(query, params)

#                 columns = [
#                     "ts", "o", "h", "l", "c", "vol", "volCcy", "volCcyQuote",
#                     "ma50", "ma200", "ema12", "ema26", "macd", "macd_signal",
#                     "rsi14", "macd_histogram", "stochastic_k", "stochastic_d"
#                 ]
#                 df = pd.DataFrame(rows, columns=columns)
#                 df = df.iloc[::-1]  # –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º





#                 # actual_df = df[["ts", "c"]].copy()
#                 # actual_df.columns = ["ts", "y_true"]
#                 # actual_df["ts"] = pd.to_datetime(actual_df["ts"])

        current_timefreme=Timeframe._1min
        # –ø–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ —Ç–æ–ø
        for id_row, row in model_results_df.iterrows():
            # –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Ç–∞–π–º—Ñ—Ä–µ–π–º —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º, —Ç–æ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–¥–æ
            if current_timefreme!=row["timeframe_enum"]:
                # –µ—Å–ª–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º >= 5 –º–∏–Ω—É—Ç —Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–º –ø–µ—Ä–µ—Ä–∞—á—Å–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                if row["timeframe_enum"].minutes>= 5:
                    amount=list(range(0,int(row["timeframe_enum"].minutes),row["offset"]))
                    current_timefreme=row["timeframe_enum"]
                    df = math_candle.generate_multi_shift_features(df_1min,row["timeframe_enum"] , amount)
                    df = df.sort_values("ts").reset_index(drop=True)

            model_path=row["model_path"]
            scaler_path=row["scaler_path"]
            feature_cols = [col for col in df.columns if col not in ["ts", "offset"]]
            

            for i in range(row["horizon"]):
                X_input =df.iloc[len(df)-i-row["window_size"]:len(df)-i]
                len_ws=row["window_size"]
                if len(X_input) < len_ws:
                    print(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏: {row['model_path']}")
                    continue
                last_ts = X_input["ts"].iloc[-1]
                X_input = X_input[feature_cols].tail(len_ws).values
                y_pred = manager.predict_on_working_models(X_input,model_path,scaler_path)

                horizon=row["horizon"]/(row["tf_minutes"]/row["offset"])
                data_forcast=pd.to_datetime(last_ts,unit="ms")
                # data_forcast=data_forcast.tz_localize('UTC').tz_convert(tzlocal.get_localzone())
                data_forcast+=timedelta(minutes=row["tf_minutes"] * (horizon+1))
                model_results.append(
                {"horizon": horizon,
                "time_frame": time_frame,
                "data_forcast": data_forcast,
                "prais": float(y_pred[0]),
                "model": row["filename"]}
                )
        self.reports_servise.report_forecast(model_results)
        for i in model_results:
            print(f'–ì–æ—Ä–∏–∑–æ–Ω—Ç - {i["horizon"]}   —Ç–∞–π–º—Ñ—Ä–µ–π–º - {i["time_frame"].label}   '+
                  f'–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –¥–∞—Ç—É {i["data_forcast"]} —Ä–∞–≤–µ–Ω {i["prais"]}')
           

       


    def make_forecast(self, ts: str):
    
        manager = ModelManager()

        table_name = Coins.FET
        time_frame = Timeframe._30min

        query = f""" SELECT * FROM {table_name.value}
                    WHERE timeFrame=%s and ts<=%s
                    ORDER BY ts DESC
                    LIMIT 100;"""
        params = (time_frame.label,ts)
        
        rows = self.db.query_to_bd(query, params)

        columns = [
            "ts", "o", "h", "l", "c", "vol", "volCcy", "volCcyQuote",
            "ma50", "ma200", "ema12", "ema26", "macd", "macd_signal",
            "rsi14", "macd_histogram", "stochastic_k", "stochastic_d"
        ]
        df = pd.DataFrame(rows, columns=columns)
        df = df.iloc[::-1]  # –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º

        actual_df = df[["ts", "c"]].copy()
        actual_df.columns = ["ts", "y_true"]
        # actual_df["ts"] = pd.to_datetime(actual_df["ts"])

        ts = df["ts"].iloc[-1]
        timeframe_minutes = int(time_frame.label[:-1])

        prepared_inputs = {
            ws: df[columns[1:]].tail(ws).values
            for ws in [30, 60, 90] if len(df) >= ws
        }

        model_results = []

        for x in manager.list_models():
            window_size = x["window_size"]
            horizon = x["horizon"]
            timeframe = x["timeframe"]
            filename = x["filename"]

            if window_size not in prepared_inputs:
                print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è window_size={window_size}")
                continue

            X_input = prepared_inputs[window_size]
            y_pred = manager.predict(X_input, timeframe, window_size, horizon, filename)

            model_results.append({
                "ts": ts,
                "target_ts": ts +horizon*30*60*1000, 
                "horizon": horizon,
                "window_size": window_size,
                "y_pred": y_pred[0],
                "y_true": None,
                "model_id": filename,
                "eval_time": datetime.now()
            })

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ CSV
        csv_path = "D:/Project_programming/for_AI/Crypto_AI_Project/results/forecast_eval_history.csv"
        df_new = pd.DataFrame(model_results)
        

        if os.path.exists(csv_path):
            df_old = pd.read_csv(csv_path)

            df_combined = pd.concat([df_old, df_new], ignore_index=True)

            # –û–±–Ω–æ–≤–ª—è–µ–º y_true, –µ—Å–ª–∏ target_ts —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å ts –≤ actual_df
            actual_map = dict(zip(actual_df["ts"], actual_df["y_true"]))

            updated_count = 0
            for i, row in df_combined[df_combined["y_true"].isna()].iterrows():
                if row["target_ts"] in actual_map:
                    df_combined.at[i, "y_true"] = actual_map[row["target_ts"]]
                    updated_count += 1
            print(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: {updated_count}")
        else:
            df_combined = df_new

        df_combined.to_csv(csv_path, index=False)
        print(f"–î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã/–æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ: {csv_path}")




    def make_forecast_old(self):
        manager = ModelManager()

        table_name = Coins.FET
        time_frame = Timeframe._30min

        query = f""" SELECT * FROM {table_name.value}
                    WHERE timeFrame=%s
                    ORDER BY ts DESC
                    LIMIT 90;"""
        params = (time_frame.label,)
        
        rows = self.db.query_to_bd(query, params)

        columns = [
            "ts", "o", "h", "l", "c", "vol", "volCcy", "volCcyQuote",
            "ma50", "ma200", "ema12", "ema26", "macd", "macd_signal",
            "rsi14", "macd_histogram", "stochastic_k", "stochastic_d"
        ]
        df = pd.DataFrame(rows, columns=columns)
        df = df.iloc[::-1]  # –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º

        max_ts = df["ts"].max()  # –±–∞–∑–æ–≤–∞—è —Ç–æ—á–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤—Ö–æ–¥—ã –ø–æ –∫–∞–∂–¥–æ–º—É window_size
        prepared_inputs = {
            ws: df[columns[1:]].tail(ws).values
            for ws in [30, 60, 90] if len(df) >= ws
        }

        results = []
        timeframe_minutes = int(time_frame.label[:-1])  # '30m' ‚Üí 30

        ts = df["ts"].iloc[-1]  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ts

        for x in manager.list_models():
            window_size = x["window_size"]
            horizon = x["horizon"]
            timeframe = x["timeframe"]
            key = x["key"]
            filename=x["filename"]

            if window_size not in prepared_inputs:
                print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è window_size={window_size}")
                continue

            X_input = prepared_inputs[window_size]
            y_pred = manager.predict(X_input, timeframe, window_size, horizon,x["filename"])
            # print(y_pred)
            label = f"+{horizon * timeframe_minutes}min"

            results.append({
                "ts": ts,
                label: y_pred[0]  # –µ—Å–ª–∏ y_pred ‚Äî –º–∞—Å—Å–∏–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            })

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å DataFrame
        df_result = pd.DataFrame(results).fillna("")

        # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–º —Å—Ç–æ–ª–±—Ü—ã: —Å–Ω–∞—á–∞–ª–∞ ts, –ø–æ—Ç–æ–º –ø–æ horizon
        cols = ["ts"] + sorted([col for col in df_result.columns if col != "ts"], key=lambda x: int(x[1:-3]))
        df_result = df_result[cols]

        print(df_result)





    def load_model_and_scalers(self):
        self.ai_service.load_model_and_scalers()

    def predict_price(self, table_name: Coins, time_frame: Timeframe) -> float:
        return self.ai_service.predict_price(table_name, time_frame)

    def train_model(
        self,
        table_name: Coins,
        time_frame: Timeframe,
        limit: int = 500000,
        window_size: int = 60,
        horizon: int = 1,
    ):
        return self.ai_service.train_model(
            table_name, time_frame, limit, window_size, horizon
        )
    


            


    def ai_expirement_predictions(self):
        self.ai_service.train_model_experiment_where_predictions(
        table_name = Coins.FET,
        time_frame = Timeframe._30min,
        limit = 500000,
        window_size = 100,
        horizon = 1,
        model_path="D:/Project_programming/for_AI/Crypto_AI_Project/expipement/exp2.h5",
        scaler_path="D:/Project_programming/for_AI/Crypto_AI_Project/expipement/exp2.pkl",
        return_predictions=True,
        epochs = 50,
        learning_rate = 0.0001,
        dropout = 0.02,
        neyro = 64,
        csv_path = "D:/Project_programming/for_AI/Crypto_AI_Project/expipement/predictions2.csv",  # –ø—É—Ç—å –¥–ª—è CSV
    )

    

    def repord_db(self, table_name: Coins):
        print(f"–ú–æ–Ω–µ—Ç–∞ {table_name.value}")
        for i in Timeframe:
            query = f"""SELECT MAX(ts) AS max_ts, 
                    min(ts) AS min_ts,
                    count(ts) as count FROM {table_name.value}
                    where timeFrame=%s;"""
            params = (i.label,)
            request = self.db.query_to_bd(query, params)
            dt_max = datetime.fromtimestamp(request[0]["max_ts"] / 1000)
            dt_min = datetime.fromtimestamp(request[0]["min_ts"] / 1000)
            print(
                f'–¢–∞–π–º—Ñ—Ä–µ–π–º {i.label}   –Ω–∞—á–∞–ª–æ {dt_min}         –∫–æ–Ω–µ—Ü {dt_min}      –∫–æ–ª–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π {request[0]["count"]}'
            )

    def first_load_candles(self, coin: Coins, length_in_hours: int = 365 * 24):

        TimControl.frequency_limitation(10)
        fetcher = OkxCandlesFetcher(instId=coin.value, bar=Timeframe._1min)
        data = fetcher.fetch_candles(limit=60)
        for candle in data:
            candle["timeFrame"] = "1m"
            candle["quoteCoin"] = SettingsCoins.quote_coin()

        self.db.insert_many_candles(data, coin.value)

        for x in range(int(length_in_hours)):
            TimControl.frequency_limitation(10)
            data = fetcher.fetch_candles(
                limit=60, afterBefore=AfterBefore.After, start_time=data[-1]["ts"]
            )
            for candle in data:
                candle["timeFrame"] = "1m"
                candle["quoteCoin"] = SettingsCoins.quote_coin()

            self.db.insert_many_candles(data, coin.value)

            print(
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {x*100/length_in_hours:.3f}%. ( {x*60} –º–∏–Ω—É—Ç –∏–∑ {length_in_hours*60})"
            )

    def check_sequence_timeframes(
        self,
        table_name: Coins,
        time_frame: Timeframe,
        start_time: int,
        stop_time: int,
        return_result: bool = False,
    ):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ.
         –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            table_name (Coins): –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–Ω–µ—Ç—ã.
            time_frame (Timeframe): –¢–∞–π–º—Ñ—Ä–µ–π–º (–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è) (–Ω–∞–ø—Ä–∏–º–µ—Ä, _1m, _5m).
            start_time (int): –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ timestamp (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã).
            stop_time (int): –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ timestamp (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã).
            return_result (bool = False): –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞–¥–æ –ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –æ—Ç–≤–µ—Ç

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            output_data = {
                "missing data": 0, - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –Ω–∞—Ä—É—à–∞–±—â–∏—Ö —Ü–µ–ª–æ—Å–Ω–æ—Å—Ç—å
                "db errors": 0     - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ –ë–î
            }

        """
        import inspect

        current_frame = inspect.currentframe()
        method_name = current_frame.f_code.co_name
        class_name = self.__class__.__name__

        output_data = {"missing data": 0, "db errors": 0}

        if start_time > stop_time:
            start_time, stop_time = stop_time, start_time
        delta_candles = (stop_time - start_time) // (time_frame.minutes * 60 * 1000) + 1
        while delta_candles > 0:
            if delta_candles > 10000:
                delta_candles = 10000
            step = time_frame.minutes * 1000 * 60
            query = f""" SELECT ts FROM {table_name.value}
                    where timeFrame=%s and ts>=%s
                    ORDER BY ts ASC
                    limit %s;
            """
            params = (time_frame.label, start_time, delta_candles)
            # –ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å –≤ –±–¥ –ø–æ—Ä—Ü–∏—é –∑–∞–ø–∏—Å–µ–π
            request = self.db.query_to_bd(query, params)
            len_request = len(request)
            for i in range(len_request - 1):
                # –µ—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –∑–∞–ø–∏—Å—è–º–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç step
                # –∑–Ω–∞—á–∏—Ç –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏
                if (int(request[i + 1]["ts"]) - int(request[i]["ts"])) > step:
                    number_of_passes = int(
                        (request[i + 1]["ts"] - request[i]["ts"]) / step
                    )
                    logging.info(
                        ContextLogger.string_context()
                        + f""": 
                                 –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {time_frame.label} —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –∫–æ–ª–ª–∏—á–µ—Å—Ç–≤–µ {number_of_passes}"""
                    )
                    output_data["missing data"] += number_of_passes
                    start_time_for_reqwest = int(request[i + 1]["ts"])
                    fetcher = OkxCandlesFetcher(instId=table_name.value, bar=time_frame)
                    while number_of_passes > 0:
                        if number_of_passes > 100:
                            number_of_candles = 100
                            limit = number_of_candles
                        else:
                            number_of_candles = number_of_passes
                            limit = number_of_candles - 1

                        data_candles = fetcher.fetch_candles(
                            limit=limit,
                            afterBefore=AfterBefore.After,
                            start_time=str(start_time_for_reqwest),
                        )
                        # with open(
                        #     r"D:\Project_programming\for_AI\data_candles.json",
                        #     "w",
                        #     encoding="utf-8",
                        # ) as f:
                        #     json.dump(data_candles, f, ensure_ascii=False, indent=4)
                        # with open(
                        #     r"D:\Project_programming\for_AI\test.json",
                        #     "w",
                        #     encoding="utf-8",
                        # ) as f:
                        #     json.dump(request, f, ensure_ascii=False, indent=4)
                        for x in data_candles:
                            x["timeFrame"] = time_frame.label
                            x["quoteCoin"] = SettingsCoins.quote_coin()

                        output_data["db errors"] = self.db.insert_many_candles(
                            data_candles, table_name.value, True
                        )
                        # start_time_for_reqwest=start_time_for_reqwest+number_of_candles
                        start_time_for_reqwest -= int(
                            (number_of_candles) * time_frame.minutes * 60 * 1000
                        )
                        number_of_passes = number_of_passes - number_of_candles

            start_time = int(request[-1]["ts"])
            delta_candles = ((stop_time - start_time) / 1000) / time_frame.minutes

        if return_result:
            return output_data

    def calculation_of_indicators(self, tableName: Coins):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (MA, EMA, MACD, RSI, Stochastic)
        –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–∏ Timeframe.

        –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç—ã:
        1. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è (`MAX(ts)`), –≥–¥–µ —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä `ma200`.
        2. –û—Ç—Å—Ç—É–ø–∞–µ—Ç –Ω–∞–∑–∞–¥ –Ω–∞ 300 —Å–≤–µ—á–µ–π, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å "—Ö–≤–æ—Å—Ç" —Å –∑–∞–ø–∞—Å–æ–º (–ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ).
        3. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Ä—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (`start_ts`) –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞.
        4. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ DataFrame, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã:
        - MA50, MA200
        - EMA12, EMA26
        - MACD, MACD Signal, MACD Histogram
        - RSI14
        - Stochastic %K –∏ %D
        5. –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ (–¥–∞–Ω–Ω—ã–µ —É–∂–µ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã),
        —Ç–æ –æ–±—Ä–µ–∑–∞—é—Ç—Å—è –ø–µ—Ä–≤—ã–µ 300 —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏.
        6. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        7. –¶–∏–∫–ª –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            tableName (Coins): –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ —Å –∏–º–µ–Ω–µ–º —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

        –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
            - –î–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 200 —Å—Ç—Ä–æ–∫.
            - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (limit = 10000) –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ –ë–î.
            - –ü–æ–≤—Ç–æ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 300 —Å—Ç—Ä–æ–∫ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤.
            - –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ MAX(ts).
        """
        limit = 50000
        for tm in Timeframe:
            lastLap = False
            old_ts = 0
            counter = 0
            number_of_records = 0

            while not lastLap:
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è, –≥–¥–µ —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω ma200
                query = f"""
                    SELECT MAX(ts) AS max_ts FROM {tableName.value}
                    WHERE ma200 IS NOT NULL AND timeFrame = %s
                """
                params = (tm.label,)
                result = self.db.query_to_bd(query, params)
                last_calculated_ts = (
                    result[0]["max_ts"] if result and result[0]["max_ts"] else 0
                )

                # –û—Ç—Å—Ç—É–ø–∞–µ–º –Ω–∞–∑–∞–¥ –Ω–∞ 300 —Å–≤–µ—á–µ–π, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ö–≤–æ—Å—Ç (–µ—Å–ª–∏ –Ω–∞–¥–æ)
                start_ts = int(last_calculated_ts) - tm.minutes * 60 * 1000 * 300
                if start_ts < 0:
                    start_ts = 0

                # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø–æ—Ä—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
                query = f"""
                    SELECT * FROM {tableName.value}
                    WHERE ts >= %s AND timeFrame = %s
                    ORDER BY ts
                    LIMIT %s
                """
                params = (start_ts, tm.label, limit)
                records = self.db.query_to_bd(query, params)

                if not records:
                    break  # –Ω–µ—á–µ–≥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å

                if (len(records) < limit) or (old_ts == records[0]["ts"]):
                    lastLap = True

                old_ts = records[0]["ts"]
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤ DataFrame
                df = pd.DataFrame(records)

                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ –º–µ–Ω—å—à–µ 200, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—Ç—å –Ω–µ–ª—å–∑—è
                if len(df) < 200:
                    print(
                        f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø–æ {tm.label}"
                    )
                    # continue

                # –†–∞—Å—á—ë—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                df["ma50"] = ta.trend.sma_indicator(df["c"], window=50)
                df["ma200"] = ta.trend.sma_indicator(df["c"], window=200)
                df["ema12"] = ta.trend.ema_indicator(df["c"], window=12)
                df["ema26"] = ta.trend.ema_indicator(df["c"], window=26)
                df["macd"] = ta.trend.macd(df["c"])
                df["macd_signal"] = ta.trend.macd_signal(df["c"])
                df["macd_histogram"] = df["macd"] - df["macd_signal"]
                df["rsi14"] = ta.momentum.rsi(df["c"], window=14)

                stoch = ta.momentum.StochasticOscillator(
                    high=df["h"], low=df["l"], close=df["c"], window=14, smooth_window=3
                )
                df["stochastic_k"] = stoch.stoch()
                df["stochastic_d"] = stoch.stoch_signal()

                if start_ts > 0:
                    # –û–±—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤—ã–µ 300 —Å—Ç—Ä–æ–∫
                    df = df.iloc[300:]
                    if df.empty:
                        continue

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                result_to_save = [
                    {k: (None if pd.isna(v) else v) for k, v in row.items()}
                    for row in df.to_dict(orient="records")
                ]

                counter += 1
                number_of_records += len(df)
                logging.info(
                    ContextLogger.string_context()
                    + f"""
                –¢–∞–π–º—Ñ—Ä–µ–π–º = {tm.label},        –ø—Ä–æ—Ö–æ–¥ - {counter},          –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ - {number_of_records} –∑–∞–ø–∏—Å–µ–π"""
                )
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                self.db.insert_many_indikator(result_to_save, tableName.value)

    def recalc_timeframe(self, baseCoin: Coins, from_tf: Timeframe, to_tf: Timeframe):
        end_cikle: bool = False
        caunter = 0
        number = 0
        while end_cikle == False:
            interval_minutes = to_tf.minutes / from_tf.minutes
            if not interval_minutes.is_integer():
                print("–í—Ö–æ–¥–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã –Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã")
                return
            interval_minutes = int(interval_minutes)
            # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ts –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞, —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π ts –∫–æ–≥–¥–∞ –±—ã–ª —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –¥–∞–Ω–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
            last_ts = self.db.get_max_timestamp(
                baseCoin.value,
                timeFrame=to_tf.label,
                quoteCoin=SettingsCoins.quote_coin(),
            )

            if last_ts is None:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –≤–æ–∑—å–º—ë–º –º–∏–Ω–∏–º—É–º –ø–æ from_tf
                candles = self.db.fetch_candles_from_ts(
                    baseCoin.value, from_tf.label, 0, SettingsCoins.quote_coin(), 50000
                )
            else:
                # –ù–∞—á–∏–Ω–∞–µ–º —Å ts —Å–ª–µ–¥—É—é—â–µ–π —Å–≤–µ—á–∏
                start_ts = last_ts + interval_minutes * 60 * 1000
                candles = self.db.fetch_candles_from_ts(
                    baseCoin.value,
                    from_tf.label,
                    start_ts,
                    SettingsCoins.quote_coin(),
                    50000,
                )

                # –ï—Å–ª–∏ len(candles) —Ç–æ —Å–≤–µ—á–∞ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –Ω–µ –ø–æ–ª–Ω–∞—è –∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ –Ω–∞–¥–æ
                # —ç—Ç–æ –º–æ–¥–µ—Ç –±—ã—Ç—å –∫–æ–≥–¥–∞ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–ª 15 –º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –≤ 15 (30) –º–∏–Ω. –∏ –≤ 15 (30) –º–∏–Ω
                # –æ–ø—Ä–∞—à–∏–≤–∞–µ—à —Å–Ω–æ–≤–∞, —Ç–æ–≥–¥–∞ —Ñ–æ—Ä–º–∏–Ω—É–µ—Ç—Å—è len(candles)>1 –∞ —Ç–∞–∫ –∫–∞–∫ –º–µ—Ç–æ–¥ aggregate_candles
                # —Ç—Ä–µ–±—É–µ—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Å–ø–∏—Å–∫–∞ –≤ aggregate_candles —Ç–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ—à–∏–±–∫–µ
            if len(candles) > 1:
                # –ê–≥–≥—Ä–µ–≥–∏—Ä—É–µ–º
                aggregated_candles = Database.aggregate_candles(candles, to_tf)

                # –í—Å—Ç–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
                self.db.insert_many_candles(
                    aggregated_candles, name_table=baseCoin.value
                )
                number += 1
                print("–ü—Ä–æ—Ö–æ–¥ - " + str(number))
                caunter += len(aggregated_candles)
                if len(aggregated_candles) == 0:
                    print(f"–ü–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–æ {caunter} —Å–≤–µ—á–µ–π —Å {from_tf} –≤ {to_tf}")
                    return caunter

            else:
                print(f"–ü–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–æ {caunter} —Å–≤–µ—á–µ–π —Å {from_tf} –≤ {to_tf}")
                end_cikle = True
                return caunter

    def data_for_update(self, table_name=Coins):
        query = f"""
                    SELECT MAX(ts) AS max_ts FROM {table_name.value}
                    WHERE timeFrame= "1m"
                """
        result = self.db.query_to_bd(query, ())
        last_time_in_database = result[0]["max_ts"]

        fetcher = OkxCandlesFetcher(instId=table_name.value, bar=Timeframe._1min)
        data = fetcher.fetch_candles(limit=1)

        if last_time_in_database < int(data[0]["ts"]):
            for x in data:
                x["timeFrame"] = Timeframe._1min.label
                x["quoteCoin"] = SettingsCoins.quote_coin()

            self.db.insert_many_candles(data, table_name.value)
        query = f"""
                    SELECT ts FROM {table_name.value} 
                    where ma200 is not null and 
                    timeFrame='1m' 
                    order by ts DESC 
                    limit 1;
                    
                """
        result = self.db.query_to_bd(query, ())
        time_in_database = result[0]["ts"]

        return {
            "time_in_database": time_in_database,
            "current_time_on_the_exchange": data[0]["ts"],
        }
